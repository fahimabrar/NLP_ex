# Word Embeddings

Topic will be covered

1. One hot encoding for word
2. TF and IDF (Term Frequency and Invert Document Frequency)
3. Word2Vec embeddings
4. GloVe embeddings


Why Embeddings is necessary?

Text or word data cannot be fed directly into machine learning or deep learning models. Because machine donot understand language. It can only recoginize numbers or patterns in numbers. So the word "good" or "bad" doesnot mean anything to a machine rather than two strings. To make computer understand of words we need to convert them numeric representations.

In factor/class data (e.g. true/false, male/female) in classification problem we normally do one hot encoding. In R package the data can be converted into fators. That is a hash mappings automatically generated by R environment. 


## One hot Encoding


<img src="https://raw.githubusercontent.com/fahimabrar/Natural-Language-Processing/main/Word%20Embeddings/xsah.JPG" 
     width="300" 
     height="200" />


### Problem with one hot encoding


## Term Frequency and Inverse Document Frequency

## Word2Vec Embeddings

## GloVe embeddings

........ will be continued along with concepts and codings

